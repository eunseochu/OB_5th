{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eunseochu/OB_5th/blob/main/%E1%84%90%E1%85%A6%E1%86%A8%E1%84%89%E1%85%B3%E1%84%90%E1%85%B3_%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8_%E1%84%8B%E1%85%A7%E1%86%AB%E1%84%89%E1%85%B3%E1%86%B8%E1%84%86%E1%85%AE%E1%86%AB%E1%84%8C%E1%85%A6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **| 텍스트 분석 연습 문제**\n",
        "\n",
        "- 출처 : 캐글"
      ],
      "metadata": {
        "id": "Yw5mfB-1YfRw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. Tokenization**\n",
        "\n",
        "In the field of Natural Language Processing, tokenization basically refers to splitting up a larger body of text into smaller lines or words.\n",
        "\n",
        "There are mainly two types of tokenization :\n",
        "\n",
        "- Sentence Tokenization\n",
        "- Word Tokenization"
      ],
      "metadata": {
        "id": "zZBGXubsY6lE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import package\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize "
      ],
      "metadata": {
        "id": "zpux756aZRgB",
        "outputId": "da1c44c1-c299-4916-e142-4c22fb04ac57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sample text to perform our operations\n",
        "text = \"Hi, My name is Amartya Nambiar. I am a Computer Science Engineer. My favourite color is black\""
      ],
      "metadata": {
        "id": "-vKDqW1WZcjr"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 문장 토큰화\n",
        "sent_tokenize(text) "
      ],
      "metadata": {
        "id": "GowligokZeEA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe787e82-1443-437c-f351-6a640c8bee8a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hi, My name is Amartya Nambiar.',\n",
              " 'I am a Computer Science Engineer.',\n",
              " 'My favourite color is black']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어 토큰화, 길이 출력\n",
        "words =word_tokenize(text)   \n",
        "print(len(words))\n",
        "print(words)"
      ],
      "metadata": {
        "id": "pY1VFCkVaDrQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65479dda-edfb-4c77-a803-9085da1b4ca3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20\n",
            "['Hi', ',', 'My', 'name', 'is', 'Amartya', 'Nambiar', '.', 'I', 'am', 'a', 'Computer', 'Science', 'Engineer', '.', 'My', 'favourite', 'color', 'is', 'black']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. Stopwords & Flushing them**\n",
        "\n",
        "Stopwords are the English words which does not add much meaning to a sentence. They can safely be ignored without sacrificing the meaning of the sentence."
      ],
      "metadata": {
        "id": "unjlrUQTaiGV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords') \n",
        "from nltk.corpus import stopwords  "
      ],
      "metadata": {
        "id": "Rf5p8-7KazcD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b58e1c6f-8be3-4450-e531-a03eafaae26f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# english stopword 불러오기, 15개만 확인\n",
        "stop = stopwords.words('english')\n",
        "print(len(stop))\n",
        "print(stop[:15])"
      ],
      "metadata": {
        "id": "f8kqXiktbBSc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8887bbc-541e-46b0-ba1f-98a9bf68c88a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "179\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 필터링을 통해 text에서 stopword 제거\n",
        "clean = [i for i in words if not i in stop]      \n",
        "print(len(clean))\n",
        "print(clean)"
      ],
      "metadata": {
        "id": "CTeQujmRbPZp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50c533ac-1896-42d3-8918-7ede0e465799"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16\n",
            "['Hi', ',', 'My', 'name', 'Amartya', 'Nambiar', '.', 'I', 'Computer', 'Science', 'Engineer', '.', 'My', 'favourite', 'color', 'black']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words =word_tokenize(text.lower())         \n",
        "clean_lower = [i for i in words if not i in stop]     \n",
        "print(len(clean_lower))\n",
        "print(clean_lower)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDCuGZprF3li",
        "outputId": "342fbcdf-0b95-4b17-bf16-a90ae1f5653e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13\n",
            "['hi', ',', 'name', 'amartya', 'nambiar', '.', 'computer', 'science', 'engineer', '.', 'favourite', 'color', 'black']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# punctuation('.', ',') 제거\n",
        "import string \n",
        "punctuations = list(string.punctuation)        \n",
        "stop += punctuations                          \n",
        "words =word_tokenize(text.lower())\n",
        "clean_lower = [i for i in words if not i in stop]\n",
        "print(len(clean_lower))\n",
        "print(clean_lower)"
      ],
      "metadata": {
        "id": "lxAH2ytMb3TY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bd84f07-c53b-4478-83b4-25f5c7ef1ff9"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n",
            "['hi', 'name', 'amartya', 'nambiar', 'computer', 'science', 'engineer', 'favourite', 'color', 'black']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3. Stemming**\n",
        "\n",
        "Stemming is a technique used to extract the base form of the words by removing affixes from them. It is just like cutting down the branches of a tree to its stems. For example, the stem of the words eating, eats, eaten is eat.\n",
        "\n",
        "There are mainly two widely used Stemmer Algorithms:\n",
        "\n",
        "- Porter Stemmer (we'll work on this)\n",
        "- Lancaster Stem"
      ],
      "metadata": {
        "id": "yEAlzfMhcBcK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "EQQuNe0McNBg"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ps 객체 생성 후 stemming , example 최소 3개 임의 생성 후 시도해보기\n",
        "# example1= ['helps', 'helping', 'helped']\n",
        "\n",
        "ps = PorterStemmer()         \n",
        "example1 = ['helps','helping','helped']   \n",
        "stemmed_example = [ps.stem(i) for i in example1]\n",
        "stemmed_example          "
      ],
      "metadata": {
        "id": "hp_atqYwdkR2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c21cc6cf-1991-4e88-a063-ffa75f7d88b9"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['help', 'help', 'help']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ps.stem('happiness') # but it isn't always the best choice"
      ],
      "metadata": {
        "id": "mwMDJ3ZaduyK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "53d67017-5fd7-461a-f2da-45dc621d62b6"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'happi'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4. Parts of Speech**\n",
        "\n",
        "To know what is the context of a particular word\n",
        "\n",
        "For example : Shyam is a Proper Noun, Desk is a Noun and Happy is an adjective."
      ],
      "metadata": {
        "id": "71eO1YE1dwBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag\n",
        "from nltk.corpus import movie_reviews\n",
        "nltk.download('movie_reviews')\n",
        "text = movie_reviews.raw(\"neg/cv954_19932.txt\") "
      ],
      "metadata": {
        "id": "S2iVOPi4eEKC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fb61013-eb51-4efd-b09c-cd982447fd4b"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# apply pos_tag(), print result\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "pos = pos_tag(word_tokenize(text))\n",
        "pos[1:5]"
      ],
      "metadata": {
        "id": "vcWgmScAedLq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0a0caf9-f094-4e92-b908-ac26cab1b617"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('new', 'JJ'), ('entry', 'NN'), ('in', 'IN'), ('the', 'DT')]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5. Lemmatization**\n",
        "\n",
        "PorterStemmer class chops off the suffixes from the word but this isn't the best thing to apply to clean our data.\n",
        "\n",
        "Stemming technique only looks at the form of the word whereas Lemmatization technique looks at the meaning of the word. It means after applying lemmatization, we will always get a valid word."
      ],
      "metadata": {
        "id": "NrFml-IJepQ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import package\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "xWF0Ibznetk6"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#lemmatize 'believes', 'happiness'\n",
        "lem = WordNetLemmatizer()\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "C-abWCqffiwP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3db69bb3-8460-4edd-c748-8dfccec81a75"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lem.lemmatize('believes')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "AQAd2pQXHaUB",
        "outputId": "772c9b6f-92ab-4414-fd8d-6dea6d581e30"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'belief'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lem.lemmatize('happiness')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "4tuJK7_mHcdO",
        "outputId": "fb987037-4811-4fe5-c270-82b5373f41a4"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'happiness'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    }
  ]
}